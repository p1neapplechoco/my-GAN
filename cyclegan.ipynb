{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8295bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "CROP_SIZE = 256\n",
    "\n",
    "NUM_CHANELS = 3\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "NUM_FEATURES = 64\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "CHECKPOINT = \"cyclegan_epoch005.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dir = \"./data/day\"\n",
    "night_dir = \"./data/night\"\n",
    "\n",
    "plt.figure()\n",
    "img = plt.imread(night_dir + '/' + os.listdir(night_dir)[0])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('sample image')\n",
    "print(f'Image dimensions {img.shape}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ba07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        v2.ToTensor(),\n",
    "        v2.Resize((IMG_SIZE, IMG_SIZE), transforms.InterpolationMode.BICUBIC),\n",
    "        v2.RandomCrop((CROP_SIZE, CROP_SIZE)),\n",
    "        v2.AutoAugment(),\n",
    "        v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class UnpairedImageDataset(Dataset):\n",
    "    def __init__(self, root_day, root_night, transform=None):\n",
    "        # grab all image paths\n",
    "        self.files_day   = sorted(glob.glob(f\"{root_day}/*\"))\n",
    "        self.files_night = sorted(glob.glob(f\"{root_night}/*\"))\n",
    "        self.transform   = transform\n",
    "        # define length as max so each epoch sees all of both domains\n",
    "        self.length      = max(len(self.files_day), len(self.files_night))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # cycle through day images sequentially\n",
    "        path_day = self.files_day[idx % len(self.files_day)]\n",
    "        # sample a random night image\n",
    "        path_night = random.choice(self.files_night)\n",
    "\n",
    "        img_day   = Image.open(path_day).convert(\"RGB\")\n",
    "        img_night = Image.open(path_night).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img_day   = self.transform(img_day)\n",
    "            img_night = self.transform(img_night)\n",
    "\n",
    "        return {\"day\": img_day, \"night\": img_night}\n",
    "\n",
    "dataset = UnpairedImageDataset(day_dir, night_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(batch[\"day\"].shape, batch[\"night\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9890c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransposeConv: out = (in + 2 x padding - kernel_size - output_padding) / stride + 1\n",
    "# Conv: out = (in - 1) x stride - 2 x padding + kernel_size + output_padding\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_blocks=6) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        n_blocks = n_blocks\n",
    "\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_CHANELS,\n",
    "                out_channels=NUM_FEATURES,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 2 downsampling layers\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_FEATURES,\n",
    "                out_channels=NUM_FEATURES * 2,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_FEATURES * 2,\n",
    "                out_channels=NUM_FEATURES * 4,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        for _ in range(n_blocks):\n",
    "            model += [ResidualBlock(NUM_FEATURES * 4)]\n",
    "\n",
    "        model += [\n",
    "            # 2 unsampling layers\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=NUM_FEATURES * 4,\n",
    "                out_channels=NUM_FEATURES * 2,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=NUM_FEATURES * 2,\n",
    "                out_channels=NUM_FEATURES,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # This layer to get RGB image\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_FEATURES,\n",
    "                out_channels=NUM_CHANELS,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransposeConv: out = (in + 2 x padding - kernel_size - output_padding) / stride + 1\n",
    "# Conv: out = (in - 1) x stride - 2 x padding + kernel_size + output_padding\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_CHANELS,\n",
    "                out_channels=NUM_FEATURES,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 2nd layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_FEATURES,\n",
    "                out_channels=NUM_FEATURES * 2,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 3rd layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_FEATURES * 2,\n",
    "                out_channels=NUM_FEATURES * 4,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 4th layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_FEATURES * 4,\n",
    "                out_channels=NUM_FEATURES * 8,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(NUM_FEATURES * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 5th (final) layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=NUM_FEATURES * 8,\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10080536",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "D = Discriminator().to(device)\n",
    "x = batch[\"day\"].to(device)        # [B, 3, 256, 256]\n",
    "pred = D(x)                        # → [B, 1, 30, 30] for 256×256 input\n",
    "\n",
    "print(x.shape, \": D(x) →\", pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ——— Hyper‑params ———\n",
    "NUM_EPOCHS   = 10\n",
    "LR           = 2e-4\n",
    "BETA1, BETA2 = 0.5, 0.999\n",
    "LAMBDA_CYCLE = 10\n",
    "\n",
    "# ——— Losses ———\n",
    "adv_loss   = nn.BCELoss()\n",
    "cycle_loss = nn.L1Loss()\n",
    "\n",
    "# ——— Models ———\n",
    "G   = Generator(9).to(device)  # X→Y\n",
    "F   = Generator(9).to(device)  # Y→X\n",
    "\n",
    "if CHECKPOINT is not None:\n",
    "    ckpt = torch.load(f\"./checkpoints/{CHECKPOINT}\", map_location=device)\n",
    "    G.load_state_dict(ckpt[\"G\"])\n",
    "    F.load_state_dict(ckpt[\"F\"])\n",
    "    \n",
    "D_Y = Discriminator().to(device)\n",
    "D_X = Discriminator().to(device)\n",
    "\n",
    "# ——— Optimizers ———\n",
    "opt_G  = optim.Adam(itertools.chain(G.parameters(), F.parameters()), lr=LR, betas=(BETA1, BETA2))\n",
    "opt_DY = optim.Adam(D_Y.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "opt_DX = optim.Adam(D_X.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "\n",
    "# ——— Logging setup ———\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "writer = SummaryWriter(\"runs/CycleGAN\")  # optional TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch, G, F, dataloader, device):\n",
    "    # grab one batch to visualize\n",
    "    batch = next(iter(dataloader))\n",
    "    real_X, real_Y = batch[\"day\"].to(device), batch[\"night\"].to(device)\n",
    "\n",
    "    fake_Y = G(real_X)\n",
    "    fake_X = F(real_Y)\n",
    "    rec_X  = F(fake_Y)\n",
    "    rec_Y  = G(fake_X)\n",
    "\n",
    "    # make a 2×3 grid: [ real_X | fake_Y | rec_X ]\n",
    "    #                  [ real_Y | fake_X | rec_Y ]\n",
    "    row1 = torch.cat([real_X, fake_Y, rec_X], dim=3)\n",
    "    row2 = torch.cat([real_Y, fake_X, rec_Y], dim=3)\n",
    "    grid = torch.cat([row1, row2], dim=2)\n",
    "\n",
    "    # save to disk\n",
    "    save_image(grid, f\"samples/epoch_{epoch:03d}.png\",\n",
    "               normalize=True, value_range=(0,1))  # or (-1,1) if you scaled that way\n",
    "\n",
    "    # log to TensorBoard\n",
    "    writer.add_image(\"CycleGAN/Results\", grid, epoch, dataformats=\"NCHW\")\n",
    "\n",
    "\n",
    "# ——— Training loop ———\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    for batch in loop:\n",
    "        real_X = batch[\"day\"].to(device)\n",
    "        real_Y = batch[\"night\"].to(device)\n",
    "\n",
    "        # — Generator step —\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        fake_Y = G(real_X)\n",
    "        fake_X = F(real_Y)\n",
    "\n",
    "        # adversarial\n",
    "        pred_fake_Y = torch.sigmoid(D_Y(fake_Y))\n",
    "        pred_fake_X = torch.sigmoid(D_X(fake_X))\n",
    "        valid_Y = torch.ones_like(pred_fake_Y)\n",
    "        valid_X = torch.ones_like(pred_fake_X)\n",
    "\n",
    "        loss_GAN_XY = adv_loss(pred_fake_Y, valid_Y)\n",
    "        loss_GAN_YX = adv_loss(pred_fake_X, valid_X)\n",
    "\n",
    "        # cycle\n",
    "        rec_X = F(fake_Y)\n",
    "        rec_Y = G(fake_X)\n",
    "        loss_cycle = (cycle_loss(rec_X, real_X) + cycle_loss(rec_Y, real_Y)) * LAMBDA_CYCLE\n",
    "\n",
    "        # identity\n",
    "        idt_Y = G(real_Y)\n",
    "        idt_X = F(real_X)\n",
    "        loss_idt = (cycle_loss(idt_Y, real_Y) + cycle_loss(idt_X, real_X)) * (LAMBDA_CYCLE * 0.5)\n",
    "\n",
    "        # total\n",
    "        loss_G = loss_GAN_XY + loss_GAN_YX + loss_cycle + loss_idt\n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # — Discriminator Y step —\n",
    "        opt_DY.zero_grad()\n",
    "        # real\n",
    "        pred_real_Y = torch.sigmoid(D_Y(real_Y))\n",
    "        loss_D_realY = adv_loss(pred_real_Y, valid_Y)\n",
    "        # fake\n",
    "        pred_fake_Y2 = torch.sigmoid(D_Y(fake_Y.detach()))\n",
    "        loss_D_fakeY = adv_loss(pred_fake_Y2, torch.zeros_like(pred_fake_Y2))\n",
    "        loss_DY = 0.5 * (loss_D_realY + loss_D_fakeY)\n",
    "        loss_DY.backward()\n",
    "        opt_DY.step()\n",
    "\n",
    "        # — Discriminator X step —\n",
    "        opt_DX.zero_grad()\n",
    "        pred_real_X = torch.sigmoid(D_X(real_X))\n",
    "        loss_D_realX = adv_loss(pred_real_X, valid_X)\n",
    "        pred_fake_X2 = torch.sigmoid(D_X(fake_X.detach()))\n",
    "        loss_D_fakeX = adv_loss(pred_fake_X2, torch.zeros_like(pred_fake_X2))\n",
    "        loss_DX = 0.5 * (loss_D_realX + loss_D_fakeX)\n",
    "        loss_DX.backward()\n",
    "        opt_DX.step()\n",
    "\n",
    "        # — Progress bar update —\n",
    "        loop.set_postfix({\n",
    "            \"L_G\": loss_G.item(),\n",
    "            \"L_DY\": loss_DY.item(),\n",
    "            \"L_DX\": loss_DX.item()\n",
    "        })\n",
    "\n",
    "    # — After each epoch: sample & checkpoint —\n",
    "    sample_images(epoch, G, F, dataloader, device)\n",
    "\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"G\": G.state_dict(),\n",
    "        \"F\": F.state_dict(),\n",
    "        \"D_X\": D_X.state_dict(),\n",
    "        \"D_Y\": D_Y.state_dict(),\n",
    "        \"opt_G\": opt_G.state_dict(),\n",
    "        \"opt_DX\": opt_DX.state_dict(),\n",
    "        \"opt_DY\": opt_DY.state_dict(),\n",
    "    }\n",
    "    torch.save(ckpt, f\"checkpoints/cyclegan_epoch{epoch:03d}.pt\")\n",
    "\n",
    "    print(f\"=> Saved samples/epoch_{epoch:03d}.png and checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Recreate your model and load weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2) Prepare your input image\n",
    "transform_infer = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(CROP_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "])\n",
    "\n",
    "def denormalize(tensor):\n",
    "    # move from [-1,1] back to [0,1]\n",
    "    return (tensor * 0.5) + 0.5\n",
    "\n",
    "def infer(path, model, device):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    x   = transform_infer(img).unsqueeze(0).to(device)      # [1,3,H,W]\n",
    "    with torch.no_grad():\n",
    "        y = model(x)\n",
    "    y = y.cpu().squeeze(0)                                  # [3,H,W]\n",
    "    return denormalize(y).permute(1,2,0).numpy()            # H×W×3 numpy\n",
    "\n",
    "# 3) Run and display\n",
    "input_path  = \"data/day/0.jpg\"\n",
    "out_day2night = infer(input_path, G, device)\n",
    "out_cycleback = infer(input_path, F, device)  # if you want F(day→night→day)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Day→Night\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(out_day2night)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Night→Day (cycle)\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(out_cycleback)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
